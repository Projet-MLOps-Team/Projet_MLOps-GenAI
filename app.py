# ========================================
# ONGLET 2: ENTRA√éNEMENT DES MOD√àLES
# ========================================
with tab2:
    st.header("Entra√Ænement des Mod√®les")
    
    # V√©rification que les donn√©es sont charg√©es
    if st.session_state.df is None:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger les donn√©es dans l'onglet 1 (EDA)")
    else:
        # Information sur les mod√®les test√©s
        st.info("""
        **3 Algorithmes de classification test√©s:**
        - Logistic Regression (R√©gression Logistique)
        - Decision Tree (Arbre de D√©cision)
        - Random Forest (For√™t Al√©atoire)
        
        Le meilleur mod√®le sera s√©lectionn√© automatiquement selon le score F1.
        """)
        
        # Bouton de lancement de l'entra√Ænement
        if st.button("üöÄ Lancer l'Entra√Ænement", type="primary"):
            with st.spinner("Entra√Ænement en cours... Veuillez patienter."):
                results, best_name = train_models(st.session_state.df)
                st.session_state.training_results = {
                    'results': results,
                    'best': best_name
                }
            st.success(f"‚úÖ Entra√Ænement termin√©! Meill# -*- coding: utf-8 -*-
"""
Application MLOps End-to-End - Pr√©diction de D√©faut de Cr√©dit
Projet Master - Universit√© Paris 1 Panth√©on-Sorbonne
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import time
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# ========================================
# CONFIGURATION DES CL√âS API
# ========================================
# ========================================
# CONFIGURATION DES CL√âS API
# ========================================
# Tentative de chargement depuis st.secrets (Streamlit Cloud) ou .env (local)
try:
    if hasattr(st, 'secrets') and ('OPENAI_API_KEY' in st.secrets or 'TAVILY_API_KEY' in st.secrets):
        OPENAI_API_KEY = st.secrets.get('OPENAI_API_KEY', '')
        TAVILY_API_KEY = st.secrets.get('TAVILY_API_KEY', '')
        MLFLOW_TRACKING_URI = st.secrets.get('MLFLOW_TRACKING_URI', 'http://localhost:5000')
    else:
        # Chargement depuis fichier .env
        from dotenv import load_dotenv
        load_dotenv()
        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
        TAVILY_API_KEY = os.getenv('TAVILY_API_KEY', '')
        MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5000')
except Exception as e:
    st.warning(f"Erreur lors du chargement des variables d'environnement: {e}")
    OPENAI_API_KEY = ''
    TAVILY_API_KEY = ''
    MLFLOW_TRACKING_URI = 'http://localhost:5000'

# Importation des clients API (OpenAI et Tavily)
try:
    from openai import OpenAI
    from tavily import TavilyClient
except ImportError:
    st.error("Veuillez installer les d√©pendances: pip install openai tavily-python")
    st.stop()

# ========================================
# FONCTION RAG : RECHERCHE WEB + SYNTH√àSE
# ========================================
# ========================================
# FONCTION RAG : RECHERCHE WEB + SYNTH√àSE
# ========================================
def rag_recherche_web(question: str, max_results: int = 5, region: str = "fr", language: str = "fr"):
    """
    Effectue une recherche web via Tavily et g√©n√®re une synth√®se via OpenAI (RAG).
    
    Param√®tres:
        question (str): La question pos√©e par l'utilisateur
        max_results (int): Nombre maximum de r√©sultats web √† r√©cup√©rer
        region (str): R√©gion de recherche (fr, eu, us, global)
        language (str): Langue des r√©sultats (fr, en)
    
    Retourne:
        answer (str): R√©ponse synth√©tis√©e par GPT
        sources (list): Liste des URLs sources utilis√©es
    """
    # V√©rification de la question
    if not question.strip():
        return "Veuillez saisir une question.", []
    
    # V√©rification des cl√©s API
    if not TAVILY_API_KEY:
        return "‚ùå Cl√© Tavily manquante (TAVILY_API_KEY non trouv√©e).", []
    if not OPENAI_API_KEY:
        return "‚ùå Cl√© OpenAI manquante (OPENAI_API_KEY non trouv√©e).", []

    # Initialisation du client Tavily pour la recherche web
    tavily = TavilyClient(api_key=TAVILY_API_KEY)
    try:
        # √âtape 1: RETRIEVAL - Recherche sur le web
        res = tavily.search(
            query=question,
            max_results=max_results,
            include_answer=False,
            search_depth="advanced"
        )
    except Exception as e:
        return f"Erreur lors de la recherche Tavily : {e}", []

    # Extraction du contenu et des sources depuis les r√©sultats
    results = res.get("results", []) if isinstance(res, dict) else []
    contexts, sources = [], []
    for r in results:
        content = r.get("content") or r.get("snippet") or ""
        url = r.get("url") or r.get("source") or ""
        if content:
            contexts.append(content)
        if url:
            sources.append(url)

    # Construction du bloc de contexte pour le prompt
    context_block = "\n\n---\n".join(contexts[:max_results]) if contexts else "Aucun contexte trouv√©."

    # Initialisation du client OpenAI pour la g√©n√©ration
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Construction du prompt pour GPT
    prompt = f"""
Tu es un assistant expert en risque de cr√©dit.
En te basant UNIQUEMENT sur le contexte ci-dessous, r√©dige une r√©ponse claire et concise.
- Langue : fran√ßais
- Format : 3 √† 5 points cl√©s maximum
- Ne g√©n√®re pas de fausses informations
- Termine par une section "Sources" si disponible.

Question :
{question}

Contexte :
{context_block}
"""
    try:
        # √âtape 2: GENERATION - Synth√®se via GPT
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        answer = resp.choices[0].message.content.strip()
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration OpenAI : {e}", sources

    return answer, sources[:max_results]

# ========================================
# CONFIGURATION DE LA PAGE STREAMLIT
# ========================================
# ========================================
# CONFIGURATION DE LA PAGE STREAMLIT
# ========================================
st.set_page_config(
    page_title="MLOps End-to-End",
    page_icon="üè¶",
    layout="wide"
)

# ========================================
# INITIALISATION DES √âTATS DE SESSION
# ========================================
# Stockage des pr√©dictions effectu√©es
if 'predictions' not in st.session_state:
    st.session_state.predictions = []

# Stockage du dataframe charg√©
if 'df' not in st.session_state:
    st.session_state.df = None

# Stockage des r√©sultats d'entra√Ænement
if 'training_results' not in st.session_state:
    st.session_state.training_results = None

# ========================================
# FONCTIONS UTILITAIRES
# ========================================
# ========================================
# FONCTIONS UTILITAIRES
# ========================================

@st.cache_data
def load_data():
    """
    Charge les donn√©es de pr√™t depuis le fichier CSV.
    Calcule √©galement le ratio dette/revenu (debt_ratio).
    
    Retourne:
        df (DataFrame): Donn√©es de pr√™t avec la colonne debt_ratio ajout√©e
    """
    df = pd.read_csv('Loan_Data.csv')
    # Calcul du ratio dette/revenu
    df['debt_ratio'] = df['total_debt_outstanding'] / df['income']
    return df

def train_models(df):
    """
    Entra√Æne 3 mod√®les de classification (Logistic Regression, Decision Tree, Random Forest).
    S√©lectionne le meilleur mod√®le bas√© sur le score F1.
    Sauvegarde le meilleur mod√®le, le scaler et les noms de features.
    
    Param√®tres:
        df (DataFrame): Donn√©es d'entra√Ænement
    
    Retourne:
        results (dict): Dictionnaire contenant les m√©triques de chaque mod√®le
        best_name (str): Nom du meilleur mod√®le
    """
    # Pr√©paration des features et de la cible
    X = df.drop(['default', 'customer_id'], axis=1)
    y = df['default']
    
    # Division des donn√©es: 60% train, 20% validation, 20% test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    
    # Normalisation des features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # D√©finition des 3 mod√®les √† tester
    models = {
        "Logistic Regression": LogisticRegression(
            max_iter=1000, random_state=42, class_weight='balanced'
        ),
        "Decision Tree": DecisionTreeClassifier(
            random_state=42, max_depth=10, class_weight='balanced'
        ),
        "Random Forest": RandomForestClassifier(
            n_estimators=100, random_state=42, max_depth=15, 
            class_weight='balanced'
        )
    }
    
    results = {}
    progress = st.progress(0)
    
    # Entra√Ænement et √©valuation de chaque mod√®le
    for i, (name, model) in enumerate(models.items()):
        # Entra√Ænement
        model.fit(X_train_scaled, y_train)
        
        # Pr√©dictions sur l'ensemble de validation
        y_pred = model.predict(X_val_scaled)
        y_proba = model.predict_proba(X_val_scaled)[:, 1]
        
        # Calcul des m√©triques
        results[name] = {
            'accuracy': accuracy_score(y_val, y_pred),
            'f1': f1_score(y_val, y_pred),
            'auc': roc_auc_score(y_val, y_proba)
        }
        
        # Mise √† jour de la barre de progression
        progress.progress((i + 1) / len(models))
    
    # S√©lection du meilleur mod√®le (bas√© sur F1 score)
    best_name = max(results, key=lambda x: results[x]['f1'])
    best_model = models[best_name]
    
    # R√©entra√Ænement du meilleur mod√®le sur l'ensemble train complet
    best_model.fit(X_train_scaled, y_train)
    
    # Sauvegarde des artefacts
    os.makedirs('artifacts', exist_ok=True)
    joblib.dump(best_model, 'artifacts/best_model.pkl')
    joblib.dump(scaler, 'artifacts/scaler.pkl')
    joblib.dump(X.columns.tolist(), 'artifacts/feature_names.pkl')
    
    return results, best_name

@st.cache_resource
def load_model():
    """
    Charge le mod√®le sauvegard√©, le scaler et les noms de features.
    
    Retourne:
        model: Mod√®le ML charg√©
        scaler: StandardScaler charg√©
        features: Liste des noms de features
    """
    try:
        model = joblib.load('artifacts/best_model.pkl')
        scaler = joblib.load('artifacts/scaler.pkl')
        features = joblib.load('artifacts/feature_names.pkl')
        return model, scaler, features
    except:
        return None, None, None

# ========================================
# INTERFACE UTILISATEUR PRINCIPALE
# ========================================
# ========================================
# INTERFACE UTILISATEUR PRINCIPALE
# ========================================

# En-t√™te de l'application
st.title("üè¶ MLOps End-to-End: Pr√©diction D√©faut Cr√©dit")
st.markdown("**Pipeline Complet:** EDA ‚Üí Training ‚Üí Prediction ‚Üí Recherche Web ‚Üí Monitoring")
st.markdown("---")

# ========================================
# CR√âATION DES ONGLETS
# ========================================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä 1. EDA",
    "ü§ñ 2. Training", 
    "üîÆ 3. Prediction",
    "üîé 4. Recherche Web",
    "üìà 5. Monitoring"
])

# ========================================
# ONGLET 1: ANALYSE EXPLORATOIRE DES DONN√âES
# ========================================
# ========================================
# ONGLET 1: ANALYSE EXPLORATOIRE DES DONN√âES
# ========================================
with tab1:
    st.header("Analyse Exploratoire des Donn√©es")
    
    # Bouton de chargement des donn√©es
    if st.button("üîÑ Charger les Donn√©es", type="primary"):
        st.session_state.df = load_data()
        st.success("‚úÖ Donn√©es charg√©es avec succ√®s!")
    
    # Affichage si les donn√©es sont charg√©es
    if st.session_state.df is not None:
        df = st.session_state.df
        
        # Affichage des m√©triques principales
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Lignes", f"{df.shape[0]:,}")
        col2.metric("Colonnes", df.shape[1])
        col3.metric("D√©fauts", f"{df['default'].sum():,}")
        col4.metric("Taux D√©faut", f"{df['default'].mean():.1%}")
        
        # Aper√ßu des donn√©es brutes
        with st.expander("üìã Voir les donn√©es"):
            st.dataframe(df.head(20), use_container_width=True)
        
        # Statistiques descriptives
        with st.expander("üìä Statistiques"):
            st.dataframe(df.describe(), use_container_width=True)
        
        # Section Visualisations
        st.subheader("Visualisations")
        col1, col2 = st.columns(2)
        
        # Graphique 1: Distribution des d√©fauts (Pie chart)
        with col1:
            fig = px.pie(
                df, names='default',
                title='Distribution des D√©fauts',
                color_discrete_sequence=['#51cf66', '#ff6b6b']
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Graphique 2: Distribution du score FICO
        with col2:
            fig = px.histogram(
                df, x='fico_score', color='default',
                title='Distribution FICO Score',
                nbins=30
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Matrice de corr√©lation
        st.subheader("Matrice de Corr√©lation")
        corr = df.drop('customer_id', axis=1).corr()
        fig = px.imshow(
            corr, text_auto='.2f',
            title="Corr√©lations entre les variables",
            color_continuous_scale='RdBu'
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("üëÜ Cliquez sur le bouton ci-dessus pour charger les donn√©es")

# ========================================
# ONGLET 2: ENTRA√éNEMENT DES MOD√àLES
# ========================================
with tab2:
    st.header("Entra√Ænement des Mod√®les")
    
    if st.session_state.df is None:
        st.warning("‚ö†Ô∏è Chargez d'abord les donn√©es (Tab 1)")
    else:
        st.info("""
        **3 Algorithmes:**
        - Logistic Regression
        - Decision Tree
        - Random Forest
        """)
        
        if st.button("üöÄ Lancer l'Entra√Ænement", type="primary"):
            with st.spinner("Entra√Ænement en cours..."):
                results, best_name = train_models(st.session_state.df)
                st.session_state.training_results = {
                    'results': results,
                    'best': best_name
                }
            st.success(f"‚úÖ Meilleur mod√®le: **{best_name}**")
            st.balloons()
        
        if st.session_state.training_results:
            results = st.session_state.training_results['results']
            best = st.session_state.training_results['best']
            
            df_results = pd.DataFrame(results).T
            df_results = df_results.round(4)
            
            st.subheader("R√©sultats")
            st.dataframe(df_results, use_container_width=True)
            
            fig = go.Figure()
            for metric in ['accuracy', 'f1', 'auc']:
                fig.add_trace(go.Bar(
                    name=metric.upper(),
                    x=list(results.keys()),
                    y=[results[m][metric] for m in results]
                ))
            fig.update_layout(title="Comparaison", barmode='group')
            st.plotly_chart(fig, use_container_width=True)
            
            st.success(f"üèÜ Meilleur: **{best}** (F1={results[best]['f1']:.4f})")

# ===== TAB 3: PREDICTION =====
with tab3:
    st.header("Pr√©diction de D√©faut")
    
    model, scaler, features = load_model()
    
    if model is None:
        st.warning("‚ö†Ô∏è Entra√Ænez d'abord un mod√®le (Tab 2)")
    else:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Informations Client")
            credit_lines = st.number_input("Lignes de cr√©dit", 0, 50, 5)
            loan_amt = st.number_input("Pr√™t (‚Ç¨)", 0, 1000000, 15000, 1000)
            total_debt = st.number_input("Dette totale (‚Ç¨)", 0, 1000000, 25000, 1000)
            income = st.number_input("Revenu (‚Ç¨)", 1000, 1000000, 60000, 1000)
        
        with col2:
            st.subheader("Profil Financier")
            years = st.number_input("Ann√©es emploi", 0, 50, 10)
            fico = st.number_input("Score FICO", 300, 850, 720)
            debt_ratio = total_debt / income
            st.metric("Debt Ratio", f"{debt_ratio:.2%}")
        
        if st.button("üîÆ Pr√©dire", type="primary", use_container_width=True):
            start = time.time()
            
            features_array = np.array([[
                credit_lines, loan_amt, total_debt, 
                income, years, fico, debt_ratio
            ]])
            features_scaled = scaler.transform(features_array)
            
            pred = int(model.predict(features_scaled)[0])
            prob = float(model.predict_proba(features_scaled)[0][1])
            pred_time = time.time() - start
            
            st.session_state.predictions.append({
                'timestamp': datetime.now(),
                'prediction': pred,
                'probability': prob,
                'time': pred_time
            })
            
            if pred == 1:
                st.error(f"### ‚ö†Ô∏è RISQUE DE D√âFAUT")
                st.write(f"**Probabilit√©:** {prob*100:.1f}%")
            else:
                st.success(f"### ‚úÖ PAS DE RISQUE")
                st.write(f"**Probabilit√©:** {prob*100:.1f}%")
            
            st.info(f"‚è±Ô∏è Temps: {pred_time*1000:.2f}ms")

# ===== TAB 4: RECHERCHE WEB (RAG) =====
with tab4:
    st.header("üîé Recherche Web (RAG) ‚Äî Explications & Contexte")
    st.caption("Utilise Tavily (recherche) + OpenAI (synth√®se) pour expliquer un r√©sultat ou trouver des infos r√©centes.")

    if not TAVILY_API_KEY or not OPENAI_API_KEY:
        st.error("API keys manquants. Ajoute `OPENAI_API_KEY` et `TAVILY_API_KEY` dans st.secrets ou .env.")
    else:
        exemples = [
            "Pourquoi le risque de d√©faut est faible pour un client avec FICO 720 et debt ratio 36% en France ?",
            "Taux de d√©faut r√©cents sur les pr√™ts conso en Europe (sources fiables) ?",
            "Principaux facteurs qui augmentent la probabilit√© de d√©faut selon la litt√©rature.",
        ]
        with st.expander("üí° Exemples de questions", expanded=False):
            for e in exemples:
                st.write(f"- {e}")

        col_a, col_b = st.columns([3, 1])
        with col_a:
            question = st.text_area("Ta question (FR)", value="", height=100, placeholder=exemples[0])
        with col_b:
            max_results = st.slider("R√©sultats web", 1, 8, 4)
            region = st.selectbox("R√©gion", ["fr", "eu", "us", "global"], index=0)
            language = st.selectbox("Langue", ["fr", "en"], index=0)

        lancer = st.button("üîç Rechercher & Expliquer", type="primary", use_container_width=True)

        if lancer:
            with st.spinner("Recherche et synth√®se en cours..."):
                answer, sources = rag_recherche_web(
                    question=question or exemples[0],
                    max_results=max_results,
                    region=region,
                    language=language
                )
            st.markdown("### üîé R√©ponse")
            st.write(answer)

            if sources:
                st.markdown("### üîó Sources")
                for i, url in enumerate(sources, 1):
                    st.markdown(f"{i}. {url}")
            else:
                st.info("Aucune source d√©tect√©e par la recherche.")

# ===== TAB 5: MONITORING =====
with tab5:
    st.header("Monitoring des Pr√©dictions")
    
    if not st.session_state.predictions:
        st.info("Aucune pr√©diction enregistr√©e. Utilisez l'onglet Prediction.")
    else:
        df_preds = pd.DataFrame(st.session_state.predictions)
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Total Pr√©dictions", len(df_preds))
        col2.metric("D√©fauts Pr√©dits", df_preds['prediction'].sum())
        col3.metric("Temps Moyen (ms)", f"{df_preds['time'].mean()*1000:.2f}")
        
        st.subheader("Historique")
        st.dataframe(df_preds, use_container_width=True)
        
        st.subheader("Distribution Probabilit√©s")
        fig = px.histogram(df_preds, x='probability', nbins=20, title="Distribution des probabilit√©s de d√©faut")
        st.plotly_chart(fig, use_container_width=True)

# ===== Footer =====
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <strong>Projet MLOps End-to-End</strong><br>
    EDA + Training + Prediction + RAG + Monitoring
</div>
""", unsafe_allow_html=True)
