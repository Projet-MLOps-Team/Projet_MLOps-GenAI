# -*- coding: utf-8 -*-
"""
Application MLOps End-to-End - Pr√©diction de D√©faut de Cr√©dit
Projet Master - Universit√© Paris 1 Panth√©on-Sorbonne
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import time
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# Configuration des cl√©s API
try:
    if hasattr(st, 'secrets'):
        OPENAI_API_KEY = st.secrets.get('OPENAI_API_KEY', '')
        TAVILY_API_KEY = st.secrets.get('TAVILY_API_KEY', '')
    else:
        from dotenv import load_dotenv
        load_dotenv()
        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
        TAVILY_API_KEY = os.getenv('TAVILY_API_KEY', '')
except:
    OPENAI_API_KEY = ''
    TAVILY_API_KEY = ''

# Import des biblioth√®ques API (optionnel pour Outil 4)
try:
    from openai import OpenAI
    from tavily import TavilyClient
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

# Fonction RAG (Outil 4)
def rag_recherche_web(question, max_results=5):
    """Recherche web + g√©n√©ration de r√©ponse avec RAG"""
    if not RAG_AVAILABLE:
        return "Biblioth√®ques OpenAI/Tavily non install√©es.", []
    if not question.strip():
        return "Veuillez poser une question.", []
    if not TAVILY_API_KEY or not OPENAI_API_KEY:
        return "Cl√©s API manquantes (configurez .env ou secrets.toml).", []
    
    try:
        # Recherche web avec Tavily
        tavily = TavilyClient(api_key=TAVILY_API_KEY)
        res = tavily.search(query=question, max_results=max_results, search_depth="advanced")
        results = res.get("results", [])
        
        # Extraction du contenu
        contexts = [r.get("content", "") for r in results if r.get("content")]
        sources = [r.get("url", "") for r in results if r.get("url")]
        context_block = "\n\n".join(contexts[:max_results])
        
        # G√©n√©ration avec OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)
        prompt = f"""Tu es un expert en risque de cr√©dit. R√©ponds en fran√ßais (3-5 points cl√©s).

Question: {question}

Contexte:
{context_block}
"""
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        return resp.choices[0].message.content.strip(), sources[:max_results]
    except Exception as e:
        return f"Erreur: {e}", []

# Configuration Streamlit
st.set_page_config(page_title="MLOps Projet", page_icon="üè¶", layout="wide")

# Initialisation des √©tats de session
if 'df' not in st.session_state:
    st.session_state.df = None
if 'training_results' not in st.session_state:
    st.session_state.training_results = None
if 'predictions' not in st.session_state:
    st.session_state.predictions = []

# Fonction de chargement des donn√©es
@st.cache_data
def load_data():
    """Charge les donn√©es CSV et calcule debt_ratio"""
    df = pd.read_csv('Loan_Data.csv')
    df['debt_ratio'] = df['total_debt_outstanding'] / df['income']
    return df

# Fonction d'entra√Ænement
def train_models(df):
    """Entra√Æne 3 mod√®les et retourne le meilleur"""
    X = df.drop(['default', 'customer_id'], axis=1)
    y = df['default']
    
    # Split: 60% train, 20% val, 20% test
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # Mod√®les
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),
        "Decision Tree": DecisionTreeClassifier(random_state=42, max_depth=10, class_weight='balanced'),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, max_depth=15, class_weight='balanced')
    }
    
    results = {}
    progress = st.progress(0)
    
    for i, (name, model) in enumerate(models.items()):
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
        y_proba = model.predict_proba(X_val_scaled)[:, 1]
        
        results[name] = {
            'accuracy': accuracy_score(y_val, y_pred),
            'f1': f1_score(y_val, y_pred),
            'auc': roc_auc_score(y_val, y_proba)
        }
        progress.progress((i + 1) / len(models))
    
    # S√©lection du meilleur
    best_name = max(results, key=lambda x: results[x]['f1'])
    best_model = models[best_name]
    best_model.fit(X_train_scaled, y_train)
    
    # Sauvegarde
    os.makedirs('artifacts', exist_ok=True)
    joblib.dump(best_model, 'artifacts/best_model.pkl')
    joblib.dump(scaler, 'artifacts/scaler.pkl')
    joblib.dump(X.columns.tolist(), 'artifacts/feature_names.pkl')
    
    return results, best_name

# Fonction de chargement du mod√®le
@st.cache_resource
def load_model():
    """Charge le mod√®le entra√Æn√©"""
    try:
        model = joblib.load('artifacts/best_model.pkl')
        scaler = joblib.load('artifacts/scaler.pkl')
        features = joblib.load('artifacts/feature_names.pkl')
        return model, scaler, features
    except:
        return None, None, None

# Interface principale
st.title("üè¶ MLOps End-to-End: Pr√©diction de D√©faut de Cr√©dit")
st.markdown("**Pipeline Complet:** EDA ‚Üí Training ‚Üí Prediction ‚Üí Recherche Web")
st.markdown("---")

# Cr√©ation des onglets
tab1, tab2, tab3, tab4 = st.tabs(["üìä Outil 1: EDA", "ü§ñ Outil 2: Training", "üîÆ Outil 3: Prediction", "üîé Outil 4: RAG"])

# OUTIL 1: EDA
with tab1:
    st.header("Analyse Exploratoire des Donn√©es")
    
    if st.button("üîÑ Charger les Donn√©es", type="primary"):
        try:
            st.session_state.df = load_data()
            st.success("‚úÖ Donn√©es charg√©es avec succ√®s!")
        except FileNotFoundError:
            st.error("‚ùå Fichier Loan_Data.csv introuvable!")
    
    if st.session_state.df is not None:
        df = st.session_state.df
        
        # M√©triques
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Nombre de lignes", f"{df.shape[0]:,}")
        c2.metric("Nombre de colonnes", df.shape[1])
        c3.metric("Nombre de d√©fauts", f"{df['default'].sum():,}")
        c4.metric("Taux de d√©faut", f"{df['default'].mean():.1%}")
        
        # Aper√ßu des donn√©es
        with st.expander("üìã Aper√ßu des donn√©es (20 premi√®res lignes)"):
            st.dataframe(df.head(20), use_container_width=True)
        
        # Statistiques
        with st.expander("üìä Statistiques descriptives"):
            st.dataframe(df.describe(), use_container_width=True)
        
        # Visualisations
        st.subheader("Visualisations")
        c1, c2 = st.columns(2)
        
        with c1:
            fig = px.pie(df, names='default', title='R√©partition des D√©fauts', 
                        color_discrete_sequence=['#2ecc71', '#e74c3c'])
            st.plotly_chart(fig, use_container_width=True)
        
        with c2:
            fig = px.histogram(df, x='fico_score', color='default', 
                             title='Distribution du Score FICO', nbins=30)
            st.plotly_chart(fig, use_container_width=True)
        
        # Matrice de corr√©lation
        st.subheader("Matrice de Corr√©lation")
        corr = df.drop('customer_id', axis=1).corr()
        fig = px.imshow(corr, text_auto='.2f', title="Corr√©lations entre variables",
                       color_continuous_scale='RdBu_r')
        st.plotly_chart(fig, use_container_width=True)

# OUTIL 2: TRAINING
with tab2:
    st.header("Entra√Ænement des Mod√®les")
    
    if st.session_state.df is None:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger les donn√©es dans l'Outil 1")
    else:
        st.info("""
        **3 Algorithmes test√©s:**
        - Logistic Regression (R√©gression Logistique)
        - Decision Tree (Arbre de D√©cision)
        - Random Forest (For√™t Al√©atoire)
        
        Le meilleur mod√®le est s√©lectionn√© selon le score F1.
        """)
        
        if st.button("üöÄ Lancer l'Entra√Ænement", type="primary"):
            with st.spinner("Entra√Ænement en cours..."):
                results, best_name = train_models(st.session_state.df)
                st.session_state.training_results = {'results': results, 'best': best_name}
            st.success(f"‚úÖ Entra√Ænement termin√©! Meilleur mod√®le: **{best_name}**")
            st.balloons()
        
        if st.session_state.training_results:
            results = st.session_state.training_results['results']
            best = st.session_state.training_results['best']
            
            # Tableau des r√©sultats
            st.subheader("R√©sultats des Mod√®les")
            df_results = pd.DataFrame(results).T.round(4)
            st.dataframe(df_results, use_container_width=True)
            
            # Graphique comparatif
            fig = go.Figure()
            for metric in ['accuracy', 'f1', 'auc']:
                fig.add_trace(go.Bar(
                    name=metric.upper(),
                    x=list(results.keys()),
                    y=[results[m][metric] for m in results]
                ))
            fig.update_layout(title="Comparaison des Performances", barmode='group')
            st.plotly_chart(fig, use_container_width=True)
            
            st.success(f"üèÜ Meilleur mod√®le: **{best}** (F1 Score: {results[best]['f1']:.4f})")

# OUTIL 3: PREDICTION
with tab3:
    st.header("Pr√©diction de Risque de D√©faut")
    
    model, scaler, features = load_model()
    
    if model is None:
        st.warning("‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le dans l'Outil 2")
    else:
        c1, c2 = st.columns(2)
        
        with c1:
            st.subheader("üìã Informations Client")
            credit_lines = st.number_input("Nombre de lignes de cr√©dit", 0, 50, 5)
            loan_amt = st.number_input("Montant du pr√™t (‚Ç¨)", 0, 1000000, 15000, 1000)
            total_debt = st.number_input("Dette totale (‚Ç¨)", 0, 1000000, 25000, 1000)
            income = st.number_input("Revenu annuel (‚Ç¨)", 1000, 1000000, 60000, 1000)
        
        with c2:
            st.subheader("üíº Profil Financier")
            years = st.number_input("Ann√©es d'emploi", 0, 50, 10)
            fico = st.number_input("Score FICO", 300, 850, 720)
            debt_ratio = total_debt / income
            st.metric("Ratio Dette/Revenu", f"{debt_ratio:.2%}")
        
        if st.button("üîÆ Effectuer la Pr√©diction", type="primary", use_container_width=True):
            start = time.time()
            
            # Pr√©paration des features
            features_array = np.array([[credit_lines, loan_amt, total_debt, income, years, fico, debt_ratio]])
            features_scaled = scaler.transform(features_array)
            
            # Pr√©diction
            pred = int(model.predict(features_scaled)[0])
            prob = float(model.predict_proba(features_scaled)[0][1])
            pred_time = time.time() - start
            
            # Sauvegarde dans l'historique
            st.session_state.predictions.append({
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'prediction': "D√©faut" if pred == 1 else "Pas de d√©faut",
                'probabilite': f"{prob*100:.1f}%",
                'temps_ms': f"{pred_time*1000:.2f}"
            })
            
            # Affichage du r√©sultat
            if pred == 1:
                st.error("### ‚ö†Ô∏è RISQUE DE D√âFAUT D√âTECT√â")
                st.write(f"**Probabilit√© de d√©faut:** {prob*100:.1f}%")
            else:
                st.success("### ‚úÖ PAS DE RISQUE DE D√âFAUT")
                st.write(f"**Probabilit√© de d√©faut:** {prob*100:.1f}%")
            
            st.info(f"‚è±Ô∏è Temps de pr√©diction: {pred_time*1000:.2f} ms")
            
            # Historique
            if len(st.session_state.predictions) > 1:
                with st.expander("üìä Voir l'historique des pr√©dictions"):
                    st.dataframe(pd.DataFrame(st.session_state.predictions), use_container_width=True)

# OUTIL 4: RAG (Recherche Web)
with tab4:
    st.header("üîé Recherche Web avec RAG")
    st.caption("Utilise Tavily (recherche) + OpenAI (synth√®se) pour r√©pondre √† vos questions sur le risque de cr√©dit")
    
    if not RAG_AVAILABLE:
        st.error("‚ùå Biblioth√®ques non install√©es. Ex√©cutez: `pip install openai tavily-python`")
    elif not TAVILY_API_KEY or not OPENAI_API_KEY:
        st.warning("‚ö†Ô∏è Cl√©s API manquantes. Configurez `.env` ou `secrets.toml` avec OPENAI_API_KEY et TAVILY_API_KEY")
    else:
        # Exemples de questions
        exemples = [
            "Quels sont les principaux facteurs de risque de d√©faut de cr√©dit selon la litt√©rature ?",
            "Comment le score FICO impacte-t-il la probabilit√© de d√©faut ?",
            "Taux de d√©faut moyens sur les pr√™ts personnels en France en 2024 ?"
        ]
        
        with st.expander("üí° Exemples de questions"):
            for ex in exemples:
                st.write(f"‚Ä¢ {ex}")
        
        # Saisie de la question
        question = st.text_area("Posez votre question en fran√ßais", height=100, 
                               placeholder="Ex: Pourquoi un FICO de 720 indique un faible risque ?")
        
        col_a, col_b = st.columns([2, 1])
        with col_a:
            rechercher = st.button("üîç Rechercher et Expliquer", type="primary", use_container_width=True)
        with col_b:
            max_results = st.slider("Nb de sources", 3, 8, 5)
        
        if rechercher and question:
            with st.spinner("üåê Recherche en cours..."):
                answer, sources = rag_recherche_web(question, max_results)
            
            st.markdown("### üìù R√©ponse G√©n√©r√©e")
            st.write(answer)
            
            if sources:
                st.markdown("### üîó Sources Utilis√©es")
                for i, url in enumerate(sources, 1):
                    st.markdown(f"{i}. [{url}]({url})")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <strong>Projet MLOps End-to-End</strong><br>
    Master - Universit√© Paris 1 Panth√©on-Sorbonne
</div>
""", unsafe_allow_html=True)
