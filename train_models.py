# -*- coding: utf-8 -*-
"""
Outil 3: EntraÃ®nement des ModÃ¨les avec MLflow
Auteur: Jiwon
"""

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, classification_report
)
import joblib
import warnings
import os
warnings.filterwarnings('ignore')

# Configuration MLflow
mlflow.set_experiment("loan_default_prediction")
mlflow.set_tracking_uri("file:./mlruns")

print("\n" + "="*70)
print("OUTIL 3: ENTRAÃŽNEMENT DES MODÃˆLES AVEC MLFLOW")
print("="*70)

# Chargement des donnÃ©es
print("\n Chargement des donnÃ©es...")
df = pd.read_csv('Loan_Data.csv')

# CrÃ©ation de debt_ratio (comme dans EDA)
df['debt_ratio'] = df['total_debt_outstanding'] / df['income']

print(f"âœ“ DonnÃ©es chargÃ©es: {df.shape}")
print(f"âœ“ Taux de dÃ©faut: {df['default'].mean():.2%}")

# SÃ©paration features et cible
X = df.drop(['default', 'customer_id'], axis=1)
y = df['default']

print(f"\nðŸ“‹ Features utilisÃ©es:")
for i, col in enumerate(X.columns, 1):
    print(f"  {i}. {col}")

# Division Train/Val/Test (60/20/20)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print(f"\n Division des donnÃ©es:")
print(f"  Train: {len(X_train):>5} Ã©chantillons ({len(X_train)/len(X):>5.1%})")
print(f"  Val:   {len(X_val):>5} Ã©chantillons ({len(X_val)/len(X):>5.1%})")
print(f"  Test:  {len(X_test):>5} Ã©chantillons ({len(X_test)/len(X):>5.1%})")

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# CrÃ©ation du dossier artifacts
os.makedirs('artifacts', exist_ok=True)

# Sauvegarde scaler et features
joblib.dump(scaler, 'artifacts/scaler.pkl')
joblib.dump(X.columns.tolist(), 'artifacts/feature_names.pkl')

print(f"âœ“ Normalisation terminÃ©e")

def evaluate_model(y_true, y_pred, y_proba):
    """Calcule les mÃ©triques"""
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(y_true, y_proba)
    }

def train_model(name, model, params):
    """EntraÃ®ne et enregistre avec MLflow"""
    
    with mlflow.start_run(run_name=name):
        print(f"\n{'='*70}")
        print(f" EntraÃ®nement: {name}")
        print(f"{'='*70}")
        
        # Log params
        mlflow.log_params(params)
        
        # EntraÃ®nement
        model.fit(X_train_scaled, y_train)
        
        # PrÃ©dictions
        y_pred_train = model.predict(X_train_scaled)
        y_pred_val = model.predict(X_val_scaled)
        y_proba_train = model.predict_proba(X_train_scaled)[:, 1]
        y_proba_val = model.predict_proba(X_val_scaled)[:, 1]
        
        # MÃ©triques
        train_metrics = evaluate_model(y_train, y_pred_train, y_proba_train)
        val_metrics = evaluate_model(y_val, y_pred_val, y_proba_val)
        
        # Log mÃ©triques
        for metric_name, value in train_metrics.items():
            mlflow.log_metric(f"train_{metric_name}", value)
        
        for metric_name, value in val_metrics.items():
            mlflow.log_metric(f"val_{metric_name}", value)
        
        # Log modÃ¨le
        mlflow.sklearn.log_model(model, "model")
        
        # Affichage
        print(f"ðŸ“ˆ Train - Acc: {train_metrics['accuracy']:.4f} | "
              f"F1: {train_metrics['f1']:.4f} | "
              f"AUC: {train_metrics['roc_auc']:.4f}")
        print(f"ðŸ“Š Val   - Acc: {val_metrics['accuracy']:.4f} | "
              f"F1: {val_metrics['f1']:.4f} | "
              f"AUC: {val_metrics['roc_auc']:.4f}")
        
        return model, val_metrics

# Configuration des 3 modÃ¨les
print("\n" + "="*70)
print(" ENTRAÃŽNEMENT DE 3 ALGORITHMES DE CLASSIFICATION")
print("="*70)

models_config = {
    "Logistic_Regression": {
        "model": LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        ),
        "params": {"model_type": "Logistic Regression", "max_iter": 1000}
    },
    "Decision_Tree": {
        "model": DecisionTreeClassifier(
            random_state=42,
            max_depth=10,
            min_samples_split=20,
            min_samples_leaf=10,
            class_weight='balanced'
        ),
        "params": {"model_type": "Decision Tree", "max_depth": 10}
    },
    "Random_Forest": {
        "model": RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=15,
            min_samples_split=20,
            min_samples_leaf=10,
            class_weight='balanced',
            n_jobs=-1
        ),
        "params": {"model_type": "Random Forest", "n_estimators": 100}
    }
}

# EntraÃ®nement
results = {}
trained_models = {}

for name, config in models_config.items():
    trained_model, metrics = train_model(name, config["model"], config["params"])
    results[name] = metrics
    trained_models[name] = trained_model

# Meilleur modÃ¨le
best_name = max(results, key=lambda x: results[x]['f1'])
best_model = trained_models[best_name]

print(f"\n{'='*70}")
print(f" MEILLEUR MODÃˆLE: {best_name}")
print(f"{'='*70}")
print(f"F1-Score (validation): {results[best_name]['f1']:.4f}")
print(f"ROC-AUC (validation):  {results[best_name]['roc_auc']:.4f}")

# Ã‰valuation Test Set
print(f"\n{'='*70}")
print(f" Ã‰VALUATION SUR L'ENSEMBLE DE TEST")
print(f"{'='*70}")

y_pred_test = best_model.predict(X_test_scaled)
y_proba_test = best_model.predict_proba(X_test_scaled)[:, 1]
test_metrics = evaluate_model(y_test, y_pred_test, y_proba_test)

for metric, value in test_metrics.items():
    print(f"{metric.upper():12s}: {value:.4f}")

print(f"\n Rapport de classification:")
print(classification_report(y_test, y_pred_test, 
                           target_names=['Pas de dÃ©faut', 'DÃ©faut']))

# Sauvegarde
joblib.dump(best_model, 'artifacts/best_model.pkl')

with open('artifacts/model_info.txt', 'w', encoding='utf-8') as f:
    f.write(f"Meilleur modÃ¨le: {best_name}\n")
    f.write(f"F1-Score (validation): {results[best_name]['f1']:.4f}\n")
    f.write(f"F1-Score (test): {test_metrics['f1']:.4f}\n")
    f.write(f"ROC-AUC (test): {test_metrics['roc_auc']:.4f}\n")

print(f"\n{'='*70}")
print(f" FICHIERS SAUVEGARDÃ‰S DANS artifacts/:")
print(f"   âœ“ best_model.pkl")
print(f"   âœ“ scaler.pkl")
print(f"   âœ“ feature_names.pkl")
print(f"   âœ“ model_info.txt")
print(f"\n POUR VOIR LES EXPÃ‰RIENCES MLFLOW:")
print(f"   Commande: mlflow ui --port 5000")
print(f"   URL: http://localhost:5000")
print(f"{'='*70}\n")
